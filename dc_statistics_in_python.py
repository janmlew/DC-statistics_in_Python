# -*- coding: utf-8 -*-
"""DC - Statistics in Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EM0GVhcLzAJDsoRPtLlH_XYj47ouVKmj

Imports.
"""

import matplotlib.pyplot as plt
import numpy as np
# from google.colab import drive
import pandas as pd
import seaborn as sns
from scipy.stats import iqr, uniform, binom, norm, poisson, expon  # Some later exercises (eg. 12th) need this.

"""Dataloads."""

# global variables
#PATH='/content/drive/MyDrive/Colab Notebooks/Project/datasets/'
# PATH no longer used, deleted from future references.

# Google Drive mount
# drive.mount('/content/drive', force_remount=True)

# Loads
food_consumption = pd.read_csv('food_consumption.csv')
amir_deals = pd.read_csv('amir_deals.csv')
world_happiness = pd.read_csv('world_happiness.csv')

"""First exercise.

In this I calculate mean and median values for consumption in both Belgium and USA.
"""

# Filter for Belgium
be_consumption = food_consumption[food_consumption['country']=='Belgium']

# Filter for USA
usa_consumption = food_consumption[food_consumption['country']=='USA']

# Calculate mean and median consumption in Belgium
print(np.mean(be_consumption['consumption']))
print(np.median(be_consumption['consumption']))

# Calculate mean and median consumption in USA
print(np.mean(usa_consumption['consumption']))
print(np.median(usa_consumption['consumption']))

"""Second exercise.

Visualisation time!
"""

# Subset for food_category equals rice
rice_consumption = food_consumption[food_consumption['food_category']=='rice']

# Histogram of co2_emission for rice and show plot
rice_consumption['co2_emission'].hist()
plt.show()

"""Third exercise.

Let's get acquainted with numpy's linspace!
"""

# Calculate the quartiles of co2_emission
print(np.quantile(food_consumption['co2_emission'], np.linspace(0, 1, 5)))

# Calculate the quintiles of co2_emission
print(np.quantile(food_consumption['co2_emission'], np.linspace(0, 1, 6)))

# Calculate the deciles of co2_emission
print(np.quantile(food_consumption['co2_emission'], np.linspace(0, 1, 11)))

"""Fourth exercise.

Std and var, as well as some histograms.
"""

# Print variance and sd of co2_emission for each food_category
print(food_consumption.groupby('food_category')['co2_emission'].agg(['var', 'std']))

# Create histogram of co2_emission for food_category 'beef'
food_consumption[food_consumption['food_category']=='beef']['co2_emission'].hist()
# Show plot
plt.show()

# Create histogram of co2_emission for food_category 'eggs'
food_consumption[food_consumption['food_category']=='eggs' ]['co2_emission'].hist()
# Show plot
plt.show()

"""Fifth exercise.

Finding outliers using IQR (below Q1-IQR and above Q3+IQR).
"""

# Calculate total co2_emission per country: emissions_by_country
emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()
#print(emissions_by_country)

# Compute the first and third quartiles and IQR of emissions_by_country
q1 = np.quantile(emissions_by_country, 0.25)
q3 = np.quantile(emissions_by_country, 0.75)
iqr1 = q3-q1
#OR
iqr2 = iqr(emissions_by_country)

# Calculate the lower and upper cutoffs for outliers
lower = q1 - 1.5 * iqr2
upper = q3 + 1.5 * iqr2

# Subset emissions_by_country to find outliers
outliers = emissions_by_country[(emissions_by_country<lower) | (emissions_by_country>upper)]
print(outliers)

"""Sixth exercise.

Calculating probabilities: performance review of certain Amir of a certain sales team.
"""

# Count the deals for each product
#counts = amir_deals.groupby('product').size().reset_index(name='counts')
counts = amir_deals['product'].value_counts()
#print(counts)

# Calculate probability of picking a deal with each product
probs = counts/counts.sum()
print(probs)

"""Seventh exercise.

Sampling Amir's deals.
"""

# Set random seed
np.random.seed(24)

# Sample 5 deals without replacement
sample_without_replacement = amir_deals.sample(5, replace=False)
print(sample_without_replacement)

# Sample 5 deals with replacement
sample_with_replacement = amir_deals.sample(5, replace=True)
print(sample_with_replacement)

"""Ninth exercise.

Creating a probability distribution
"""

# Defining a locally needed variable
restaurant_groups = pd.DataFrame({'group_id':	['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'], 'group_size':	[2, 4, 6, 2, 2, 2, 3, 2, 4, 2]})

# Create a histogram of restaurant_groups and show plot
restaurant_groups['group_size'].hist(bins=[2, 3, 4, 5, 6])
plt.show()

print(restaurant_groups['group_size'].value_counts())

# Create probability distribution
size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups['group_size'].count()
#size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]

# Reset index and rename columns
size_dist = pd.DataFrame(data=size_dist).reset_index()
size_dist.columns = ['group_size', 'prob']

# Calculate expected value
expected_value = (size_dist['group_size'] * size_dist['prob']).sum()
#expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])
print(expected_value)

# Subset groups of size 4 or more
groups_4_or_more = size_dist[size_dist['group_size']>=4]

# Sum the probabilities of groups_4_or_more
prob_4_or_more = groups_4_or_more['prob'].sum()
print(prob_4_or_more)

"""Tenth exercise.

An exercise in continuous uniform distribution. We will be calculating the expected waiting time assuming an occurence happens every 30 minutes.
"""

# Min and max wait times for back-up that happens every 30 min
min_time = 0
max_time = 30

# Calculate probability of waiting less than 5 mins
prob_less_than_5 = uniform.cdf(5, min_time, max_time)
print(prob_less_than_5)

# Calculate probability of waiting more than 5 mins
prob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)
print(prob_greater_than_5)

# Calculate probability of waiting 10-20 mins
prob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - uniform.cdf(10, min_time, max_time)
print(prob_between_10_and_20)

"""Eleventh exercise.

Simulations in continuous uniform distribution.
"""

# Set random seed to 334
np.random.seed(334)

# Generate 1000 wait times between 0 and 30 mins
wait_times = uniform.rvs(min_time, max_time, size=1000)

# Create a histogram of simulated times and show plot
plt.hist(wait_times)
plt.show()

"""Twelfth exercise.

Simulations in binomial distribution cases.
"""

# Set random seed to 10
np.random.seed(10)

# Simulate a single deal
print(binom.rvs(1, 0.3, size=1))

# Simulate 1 week of 3 deals
print(binom.rvs(3, 0.3, size=1))

# Simulate 52 weeks of 3 deals
deals = binom.rvs(3, 0.3, size=52)

# Print mean deals won per week
print(np.mean(deals))

"""Thirteenth exercise.

Calculating binomial probabilities.
"""

# Probability of closing 3 out of 3 deals
prob_3 = binom.pmf(3, 3, 0.3)

print(prob_3)

# Probability of closing <= 1 deal out of 3 deals
prob_less_than_or_equal_1 = binom.cdf(1, 3, 0.3)

print(prob_less_than_or_equal_1)

# Probability of closing > 1 deal out of 3 deals
prob_greater_than_1 = 1 - binom.cdf(1, 3, 0.3)

print(prob_greater_than_1)

"""Fourteenth exercise.

Calculating binomial probabilities (still).

* Calculate the expected number of sales out of the 3 he works on that Amir will win each week if he maintains his 30% win rate.
* Calculate the expected number of sales out of the 3 he works on that he'll win if his win rate drops to 25%.
* Calculate the expected number of sales out of the 3 he works on that he'll win if his win rate rises to 35%.
"""

# Expected number won with 30% win rate
won_30pct = 3 * 0.3
print(won_30pct)

# Expected number won with 25% win rate
won_25pct = 3 * 0.25
print(won_25pct)

# Expected number won with 35% win rate
won_35pct = 3 * 0.35
print(won_35pct)

"""Fifteenth exercise.

Show a histogram of a worker's trades / deals.

* The distribution seems to be normal.
"""

# Histogram of amount with 10 bins and show plot
plt.hist(amir_deals['amount'], bins=10)
plt.show()
# The code below was the only one accepted by DataCamp. The result was almost the same.
amir_deals['amount'].hist(bins=10)
plt.show()

"""Sixteenth exercise.

Since each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals and follow a normal distribution with a mean of 5000 dollars and a standard deviation of 2000 dollars. As part of his performance metrics, you want to calculate the probability of Amir closing a deal worth various amounts.

So, we have here a normal distribution with:
* mean = 5000
* sd = 2000

There are four parts of this exercise:


1.   What's the probability of Amir closing a deal worth less than $7500?

2.   What's the probability of Amir closing a deal worth more than $1000?

3.   What's the probability of Amir closing a deal worth between 3000 and 7000 dollars?

4.   What amount will 25% of Amir's sales be less than?


"""

# Probability of deal < 7500
prob_less_7500 = norm.cdf(7500, 5000, 2000)
print(prob_less_7500)

# Probability of deal > 1000
prob_over_1000 = 1 - norm.cdf(1000, 5000, 2000)
print(prob_over_1000)

# Probability of deal between 3000 and 7000
prob_3000_to_7000 = norm.cdf(7000, 5000, 2000) - norm.cdf(3000, 5000, 2000)
print(prob_3000_to_7000)

# Calculate amount that 25% of deals will be less than
pct_25 = norm.ppf(0.25, 5000, 2000)
print(pct_25)

"""Simulating sales under new market conditions

The company's financial analyst is predicting that next quarter, the worth of each sale will increase by 20% and the volatility, or standard deviation, of each sale's worth will increase by 30%. To see what Amir's sales might look like next quarter under these new market conditions, you'll simulate new sales amounts using the normal distribution and store these in the new_sales DataFrame, which has already been created for you.
"""

# Calculate new average amount
new_mean = 5000*1.2

# Calculate new standard deviation
new_sd = 2000*1.3

# Simulate 36 new sales
new_sales = norm.rvs(new_mean, new_sd, size=36)

# Create histogram and show
plt.hist(new_sales)
plt.show()

"""A simulation from the video involving showing sampling distributions of different number of trials of taking an average of 5 rolls of a 6-sided die."""

# Define die:
die=pd.Series([1,2,3,4,5,6])

# 10 trials
sample_means = []
for i in range(10):
  sample_means.append(np.mean(die.sample(5, replace=True)))
plt.hist(sample_means)
plt.show()

# 100 trials
sample_means = []
for i in range(100):
  sample_means.append(np.mean(die.sample(5, replace=True)))
plt.hist(sample_means)
plt.show()

# 1000 trials
sample_means = []
# Added a plot of a sampling distribution of 1000 standard deviations on different die rolls done at the same time.
sample_sds = []
for i in range(1000):
  sample_means.append(np.mean(die.sample(5, replace=True)))
  sample_sds.append(np.std(die.sample(5, replace=True)))
plt.hist(sample_means)
plt.show()
plt.hist(sample_sds)
plt.show()

# Calculating the mean from a distribution of proportions of drawing a particular name out of a list of names.
# Variable definitions:
sales_team = pd.Series(['Amir', 'Brian', 'Claire', 'Damian'])
sample_proportions = []

# Loop:
for i in range(100):
  st_sample = sales_team.sample(100, replace = True)
  sample_proportions.append(st_sample[st_sample=='Amir'].count()/100)

# Plot:
plt.hist(sample_proportions)
plt.show()

"""Seventeenth exercise.

In this one, we will focus on the sample mean and see the central limit theorem in action while examining the num_users column of amir_deals more closely, which contains the number of people who intend to use the product Amir is selling.
"""

# Create a histogram of num_users and show
plt.hist(amir_deals['num_users'])
plt.show()

# Set seed to 104
np.random.seed(104)

# Sample 20 num_users with replacement from amir_deals
samp_20 = amir_deals['num_users'].sample(20, replace=True)

# Take mean of samp_20
print(np.mean(samp_20))

sample_means = []
# Loop 100 times
for i in range(100):
  # Take sample of 20 num_users
  samp_20 = amir_deals['num_users'].sample(20, replace=True)
  # Calculate mean of samp_20
  samp_20_mean = np.mean(samp_20)
  # Append samp_20_mean to sample_means
  sample_means.append(samp_20_mean)

print(sample_means)

# Convert to Series and plot histogram
sample_means_series = pd.Series(sample_means)
sample_means_series.hist()
# Show plot
plt.show()

"""Eighteenth exercise.

* Set the random seed to 321.
* Take 30 samples (with replacement) of size 20 from *all_deals['num_users']* and take the mean of each sample. Store the sample means in *sample_means*.
* Print the mean of *sample_means*.
* Print the mean of the *num_users* column of *amir_deals*.

I do not have the all_deals DF, so I will probably have to simulate it somehow 😀

PS. What I did was to replace all_deals DF with amir_deals DF 😆
"""

# Set seed to 321
np.random.seed(321)

sample_means = []
# Loop 30 times to take 30 means
for i in range(30):
  # Take sample of size 20 from num_users col of all_deals with replacement
  cur_sample = amir_deals['num_users'].sample(20, replace=True)
  # Take mean of cur_sample
  cur_mean = np.mean(cur_sample)
  # Append cur_mean to sample_means
  sample_means.append(cur_mean)

# Print mean of sample_means
print(np.mean(sample_means))

# Print mean of num_users in amir_deals
print(np.mean(amir_deals['num_users']))

"""Nineteenth exercise.

Tracking lead responses

Your company uses sales software to keep track of new sales leads. It organizes them into a queue so that anyone can follow up on one when they have a bit of free time. Since the number of lead responses is a countable outcome over a period of time, this scenario corresponds to a Poisson distribution. On average, Amir responds to 4 leads each day. In this exercise, you'll calculate probabilities of Amir responding to different numbers of leads.
"""

# Probability of 5 responses
prob_5 = poisson.pmf(5, 4)

print(prob_5)

# Probability of 5 responses
prob_coworker = poisson.pmf(5, 5.5)

print(prob_coworker)

# Probability of 2 or fewer responses
prob_2_or_less = poisson.cdf(2, 4)

print(prob_2_or_less)

# Probability of > 10 responses
prob_over_10 = 1 - poisson.cdf(10, 4)

print(prob_over_10)

"""Twentieth exercise.

Modeling time between leads

To further evaluate Amir's performance, you want to know how much time it takes him to respond to a lead after he opens it. On average, it takes 2.5 hours for him to respond. In this exercise, you'll calculate probabilities of different amounts of time passing between Amir receiving a lead and sending a response.

* Import expon from scipy.stats. What's the probability it takes Amir less than an hour to respond to a lead?
* What's the probability it takes Amir more than 4 hours to respond to a lead?
* What's the probability it takes Amir 3-4 hours to respond to a lead?
"""

# Print probability response takes < 1 hour
print(expon.cdf(1, scale=2.5))

# Print probability response takes > 4 hours
print(1-expon.cdf(4, scale=2.5))

# Print probability response takes 3-4 hours
print(expon.cdf(4, scale=2.5) - expon.cdf(3, scale=2.5))

"""**Uwaga!**

Muszę **koniecznie** sprawdzić dlaczego w tym przykładzie dla czasu między events wynoszącego 2.5 scale = 2.5, a nie jak w przykładzie z wykładu, gdzie dla czasu pomiędzy events równego 2, scale = 0.5.

Jest tu jakieś discrepancy.

Twenty-first exercise.

In this chapter, you'll be working with a dataset world_happiness containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.

In this exercise, you'll examine the relationship between a country's life expectancy (life_exp) and happiness score (happiness_score) both visually and quantitatively. seaborn as sns, matplotlib.pyplot as plt, and pandas as pd are loaded and world_happiness is available.

* Create a scatterplot of happiness_score vs. life_exp (without a trendline) using seaborn. Show the plot.
* Create a scatterplot of happiness_score vs. life_exp with a linear trendline using seaborn, setting ci to None. Show the plot.
* Calculate the correlation between life_exp and happiness_score. Save this as cor.
"""

# Create a scatterplot of happiness_score vs. life_exp and show
sns.scatterplot(x='life_exp', y='happiness_score', data=world_happiness)
plt.show()

# Create scatterplot of happiness_score vs life_exp with trendline
sns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)
plt.show()

# Correlation between life_exp and happiness_score
cor = world_happiness['happiness_score'].corr(world_happiness['life_exp'])
print(cor)

"""Twenty-second exercise.

What can't correlation measure?

While the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it's far from perfect. In this exercise, you'll explore one of the caveats of the correlation coefficient by examining the relationship between a country's GDP per capita (gdp_per_cap) and happiness score.

pandas as pd, matplotlib.pyplot as plt, and seaborn as sns are imported, and world_happiness is loaded.

* Create a seaborn scatterplot (without a trendline) showing the relationship between gdp_per_cap (on the x-axis) and life_exp (on the y-axis). Show the plot
* Calculate the correlation between gdp_per_cap and life_exp and store as cor.
"""

# Scatterplot of gdp_per_cap and life_exp
sns.scatterplot(x='gdp_per_cap', y='life_exp', data=world_happiness)

# Show plot
plt.show()

# Correlation between gdp_per_cap and life_exp
cor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])

print(cor)

"""Twenty-third exercise.

Transforming variables

When variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. In this exercise, you'll perform a transformation yourself.

pandas as pd, numpy as np, matplotlib.pyplot as plt, and seaborn as sns are imported, and world_happiness is loaded.

* Create a scatterplot of happiness_score versus gdp_per_cap and calculate the correlation between them.
* Add a new column to world_happiness called log_gdp_per_cap that contains the log of gdp_per_cap.
* Create a seaborn scatterplot of log_gdp_per_cap and happiness_score and calculate the correlation between them.
"""

# Scatterplot of happiness_score vs. gdp_per_cap
sns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)
plt.show()

# Calculate correlation
cor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])
print(cor)

# Create log_gdp_per_cap column
world_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])

# Scatterplot of log_gdp_per_cap and happiness_score
sns.scatterplot(y='happiness_score', x='log_gdp_per_cap', data=world_happiness)
plt.show()

# Calculate correlation
cor = world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])
print(cor)